
<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Kafka Broker Configuration</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.0/css/bootstrap.min.css" integrity="sha384-SI27wrMjH3ZZ89r4o+fGIJtnzkAnFs3E4qz9DIYioCQ5l9Rd/7UAa8DHcaL8jkWt" crossorigin="anonymous">
    <style>
      pre { background: #f5f5f5; border: 1px solid #ddd; border-radius: 4px; padding: 0.6em 1em; }
      h1,h2 { margin-top: 1em; }
      div.navbar { padding: 8px 0; }
      div.toc { float: right; }
    </style>
  </head>
  <body>
    <div class="container">
      <h1 id="kafka-broker-configuration"><a class="toclink" href="#kafka-broker-configuration">Kafka Broker Configuration</a></h1>
<h2 id="java-version"><a class="toclink" href="#java-version">Java Version</a></h2>
<ul>
<li>Recommend the latest released version of JDK 1.8, -- LinkedIn is currently running JDK 1.8 u5.</li>
</ul>
<h2 id="jvm-configuration"><a class="toclink" href="#jvm-configuration">JVM Configuration</a></h2>
<ul>
<li>
<p>Here is a sample for <code>KAFKA_JVM_PERFORMANCE_OPTS</code></p>
<p>-Xmx8g -Xms8g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80</p>
</li>
</ul>
<h2 id="deployment"><a class="toclink" href="#deployment">Deployment</a></h2>
<ul>
<li>
<p>In at least three data centers with high bandwidth and low latency between them. (Commonly, using three availability zones inside one region of a cloud provider)</p>
<ul>
<li>IMPORTANT: the <code>latency/bandwidth</code> between brokers could highly impact the <code>throughput/latency</code> of a producer client.</li>
</ul>
</li>
<li>
<p><code>rack.id</code> could be used to identify brokers from different data centers.</p>
</li>
</ul>
<h2 id="functionality"><a class="toclink" href="#functionality">Functionality</a></h2>
<ul>
<li>
<p>Controller</p>
<ul>
<li>Maintains leaders/replicas info for partitions.</li>
</ul>
</li>
<li>
<p>Partition Replicas</p>
<ul>
<li>
<p>Leader replica</p>
<ul>
<li>All produce/consume requests go through the leader.</li>
</ul>
</li>
<li>
<p>In-Sync Replica</p>
<ul>
<li>
<p>Replicas that are continuously asking for the latest messages; has caught up to the most recent message in 10 seconds (<code>replica.lag.time.max.ms</code>).</p>
</li>
<li>
<p>The preferred in-sync replica would be promoted to new leader while the previous one fails.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="os-tuning"><a class="toclink" href="#os-tuning">OS tuning</a></h2>
<p>(Use <code>sysctl</code>, or edit <code>/etc/sysctl.conf</code> for permanent change)</p>
<ul>
<li>
<p>File descriptor limits</p>
<ul>
<li>
<p><code>fs.file-max</code></p>
</li>
<li>
<p>Recommended 100000 or higher.</p>
</li>
</ul>
</li>
<li>
<p>Maximum number of memory map areas for a process</p>
<ul>
<li>
<p><code>vm.max_map_count</code></p>
</li>
<li>
<p>Each log segment uses 2 map areas.</p>
</li>
</ul>
</li>
<li>
<p>Virtual memory</p>
<ul>
<li>
<p>It's best to avoid swapping at all costs.</p>
</li>
<li>
<p>Set <code>vm.swappiness</code> to a very low value (e.g, 1).</p>
</li>
</ul>
</li>
<li>
<p>Dirty page</p>
<ul>
<li>
<p><code>vm.dirty_background_ratio=5</code>,  is appropriate in many situations.</p>
</li>
<li>
<p><code>vm.dirty_ratio=60(~80)</code>, is a reasonable number.</p>
</li>
</ul>
</li>
<li>
<p>Networking</p>
<ul>
<li>
<p><code>net.core.wmem_default</code> and <code>net.core.rmem_default</code>, reasonable setting: 131072 (128KB).</p>
</li>
<li>
<p><code>net.core.wmem_max</code> and <code>net.core.rmem_max</code>, reasonable setting: 2097152 (2MB).</p>
</li>
<li>
<p><code>net.ipv4.tcp_wmem</code> and <code>net.ipv4.tcp_rmem</code>, an example setting: 4096 (4KB minimum), 65536 (64KB default), 2048000 (2MB maximum).</p>
</li>
<li>
<p><code>net.ipv4.tcp_window_scaling=1</code>.</p>
</li>
<li>
<p><code>net.ipv4.tcp_max_syn_backlog</code>, should be above 1024 (default) to allow more simultaneous connections.</p>
</li>
<li>
<p><code>net.core.netdev_max_backlog</code>, should be above 1000 (default) to allow more packets to be queued to process.</p>
</li>
</ul>
</li>
</ul>
<h2 id="disks-and-file-system"><a class="toclink" href="#disks-and-file-system">Disks and File-system</a></h2>
<ul>
<li>
<p>Throughput of the broker disks directly influence the performance of producer clients.</p>
</li>
<li>
<p>EXT4 and XFS are the most popular choices (XFS with better performance). Some companies are even trying with ZFS.</p>
</li>
<li>
<p>Do not use mounted shared drives and any network file systems.</p>
</li>
<li>
<p>Do not share the same drives used for Kafka data with other applications to ensure good latency.</p>
</li>
</ul>
<h2 id="broker-settings"><a class="toclink" href="#broker-settings">Broker Settings</a></h2>
<ul>
<li>
<p>Auto-created Topics</p>
<ul>
<li>
<p>With <code>auto.create.topics.enable=true</code>, a topic could be created while,</p>
<ol>
<li>
<p>A producer starts writing messages to the topic.</p>
</li>
<li>
<p>A consumer starts reading messages from the topic.</p>
</li>
<li>
<p>Any client requests metadata for the topic.</p>
</li>
</ol>
</li>
<li>
<p>The auto-created topics might not be what you want</p>
<p>You might want to override some default configurations</p>
<ol>
<li>
<p><code>default.replication.factor=3</code></p>
<ul>
<li>
<p>We recommend a replication factor of 3 (at least) for any topic where availability is an issue.</p>
</li>
<li>
<p>The replication factor should be no more than the number of brokers.</p>
</li>
</ul>
</li>
<li>
<p><code>offsets.topic.replication.factor=3</code></p>
<ul>
<li>It's for the internal topic <code>__consumer_offsets</code>, -- auto-topic-creation will fail with a GROUP_COORDINATOR_NOT_AVAILABLE error if the cluster can't meet this replication factor requirement.</li>
</ul>
</li>
<li>
<p><code>num.partitions=5</code> (or whatever you want)</p>
</li>
</ol>
</li>
<li>
<p>Unclean leader election</p>
<ul>
<li>Set <code>unclean.leader.election.enable=false</code> to avoid out-of-sync replicas.</li>
</ul>
</li>
<li>
<p>Minimal in-sync replicas</p>
<ul>
<li>Set <code>min.insync.replicas=2</code> (at least) for fault-tolerant.</li>
</ul>
</li>
<li>
<p>Log</p>
<ul>
<li>
<p><code>log.retention.bytes</code> and <code>log.retention.ms/hours</code>, -- the log segment will be cleared if it exceeds the limits.</p>
</li>
<li>
<p><code>log.segment.bytes</code> and <code>log.segment.ms</code>, -- a new log segment will be created if any of the limits is reached.</p>
</li>
</ul>
</li>
<li>
<p>Threads for recovery</p>
<ul>
<li><code>num.recovery.threads.per.data.dir</code> (default 1), could be a larger number to speed up opening/closing log segments, recovering from failure, etc.</li>
</ul>
</li>
<li>
<p>Maximum message size supported</p>
<ul>
<li>
<p><code>message.max.bytes</code> (default 1000012).</p>
</li>
<li>
<p><code>replica.fetch.max.bytes</code> MUST be larger than <code>message.max.bytes</code>.</p>
</li>
<li>
<p>MUST be coordinated with (lower than) the <code>fetch.message.max.bytes</code> configuration of consumer clients.</p>
</li>
<li>
<p>MUST be coordinated (same) with the <code>message.max.bytes</code> configuration of producer clients.</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="performance-tips"><a class="toclink" href="#performance-tips">Performance tips</a></h1>
<ul>
<li>
<p>Factors: Memory, Disk, Partitions, and Ethernet bandwidth.</p>
</li>
<li>
<p>Partitions could be used to improve throughput, by using multiple Producers/Consumers.</p>
</li>
<li>
<p>Suggests that limiting the size of the partition on the disk to less than 6 GB per day of retention often gives satisfactory results.</p>
</li>
</ul>
      <hr/>
      <footer class="text-center text-muted">
        Generated: 2022. 10. 12
      </footer>
      <hr/>
    </div>
  </body>
</html>